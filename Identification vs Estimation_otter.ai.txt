Identification versus estimation

Our main contribution is not to estimate a causal effect.
It is to identify which causal effects can be estimated from data.
And the difference is important, because we cannot estimate a causal effect with the current data set.
On the other hand, by integrating our data with prior domain knowledge, we can identify which causal effects can and cannot be estimated from data.
Let me explain with a simple model.
We have a virus.
And as we have described there are measured nodes, and there are unmeasured nodes.
But we also now have confounding edges. These edges represent unmeasured common causes of nodes in these network.
And we know of the existence of these unmeasured Common causes because we have the entire Knowledge Graph of all causal relationships that have been manually curated from the literature.
And according to that knowledge graph, we know that there's some protein not in the model that regulates Bs and E.
And as a result, any estimates of the causal effect of B on E are going to be biased due to that confounder.
Now, in the potential outcomes framework, which is the main competitor to the Pearl style causal inference claim that  "All confounders must be observed."  If there's a single unmeasured confounder, then a potential outcomes person throws up his hands and says "I cannot estimate the causal effect" in the Pearl-style causal inference we say, if the causal effect is identifiable, then it can be estimated from data.
So, to give a sense of how many confounders there can be, lets look at the causal effects of the NS1 protein from the vn1203 versus Mock at 24 hours   there are 77 nodes here, of which 42 are observed.
We searched the OmniPath KB to see if there was any common cause for each pair of nodes in this model, and of the potential 2926 common causes, there were  948, almost exactly 1/3rd.
Now that's just too much to visualize.
I mean, this causal graph was already too difficult for our domain scientist to interpret because she wasn't familiar with most of the nodes in the network.
After adding 948 edges, then it becomes unreadable.
Nonetheless, despite the existence of these 948 edges, we then asked which causal effects could be identified?
So I took a topological sort of the causal model (which linearizes the DAG while preserving order) and then I asked  if we can identify when the causal effect of the upstream node on the downstream node  can be estimated from data?
And it turns out that the causal query was identifiable in 2100 of the queries, and only in 826 was the causal query not identifiable.
So, this gives us confidence that even though we have 948 confounding edges.
948 unmeasured Common causes.
Even with a density that you can't even visualize, you can still estimate the causal effect.
Now those causal effects are not as simple as a linear regression.
You have a different estimand that you have to estimate but that estimand automatically comes out of the  identification algorithm.
So not only do we know that the causal effect is identifiable, we also know how to estimate it from data.
So, that is the main contribution of our workflow. Once we have identified the causal effect, it is no longer a causal inference problem.
It now becomes a statistics problem which is something that we leave to our highly qualified statisticians.
Our contribution is the classification of the 2926 potential causal effects into 2100 identifiable causal effects that can be estimated from data given assumptions that were justified from domain knowledge and 826 nonidentifiable causal effects that cannot be estimated from data without additional assumptions.
For each of the 2100 identifiable causal effects, our statisticians can design their favorite estimator or they can use the doubly-robust estimators that automatically come out of our causal identification software.  


